{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdExvWGKYL05WYiPn3OqQn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akirakudo901/qlearning_demonstration_mint/blob/master/qlearning_demonstration_MINT_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reinforcement learning demonstration**\n",
        "This notebook demonstrates the **basic training loop** for **reinforcement learning** through **q-learning** and the **epsilon-greedy algorithm**.\n",
        "Tried to make it as brief as possible, so please ask anything you're unsure about!\n",
        "\n",
        "____________________________________________"
      ],
      "metadata": {
        "id": "ZfvmcCR02QER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **WHAT IS REINFORCEMENT LEARNING?**\n",
        "\n",
        "### The four features of reinforcement learning (RL)\n",
        "\n",
        "RL comes down to **four** features:\n",
        "* **Environment** - the world that we want to learn from. A single situation in an environment is a **\"state\"**.\n",
        "* **Agent** - the entity that learns through observing the environment and taking actions.\n",
        "* **Observation** - what the agent perceives about the world.\n",
        "* **Action** - what the agent does.\n",
        "______________________________________________\n",
        "\n",
        "### Agent-environment interaction (step)\n",
        "\n",
        "*A nice [16 second illustration](https://www.youtube.com/watch?v=-uXVu0l8guo&t=12s) of the flow on YouTube!*\n",
        "\n",
        "We represent a single **\"step\"** of agent-environment interaction as follows:\n",
        "1. the agent **observes** the environment\n",
        "2. it **chooses action** based on observation\n",
        "3. the action **modifies** the environment (& environment might also change naturally)\n",
        "\n",
        "In the next step, the agent observes the new environment state, takes action,\n",
        "which modifies the environment, and so on.\n",
        "A step is a single time step in the agent's view.\n",
        "\n",
        "______________________________________________\n",
        "\n",
        "### Reward function and policy learning\n",
        "\n",
        "The agent **learns from interacting** with the world:\n",
        "* a **\"reward function\"** inherent in the environment and hidden from the agent, evaluates **each state & action's rewarding-ness** to the agent in **numerical values**\n",
        "* the agent learns through **associating actions** taken in certain states with **resultant rewards**.\n",
        "Different algorithms (Q-learning, deep neural networks, etc.) are used for the agent's learning\n",
        "* the agent **starts off knowing nothing about the world**, and explores the world **taking random actions**\n",
        "* its goal is to **learn a guiding rule** for **action maximizing reward** at each state (called **\"policy\"**)\n",
        "\n",
        "Our goal is to let the agent **learn a highly performant policy**.\n",
        "Usual RL training involves the agent interacting a lot with the environment.\n",
        "______________________________________________\n",
        "\n",
        "### What changes from problem to problem for RL\n",
        "\n",
        "We change:\n",
        "* **ENVIRONMENT** : the world we learn from depends on the problem + (also includes observations and actions)\n",
        "  \n",
        "  <- probably the **software & signal processing team**'s field!\n",
        "\n",
        "* **REWARD FUNCTION** : what quantifies the goal of the agent\n",
        "  \n",
        "  <- probably the **whole team**'s field! (maybe more on the deep learning team?)\n",
        "\n",
        "* **AGENT'S LEARNING ALGORITHM** : the algorithm by which the agent learns\n",
        "  \n",
        "  <- probably the **deep learning team**'s field!\n",
        "\n",
        "The general flow of problem solving stays the same for most RL problems!\n",
        "\n",
        "_____________________________________"
      ],
      "metadata": {
        "id": "HFiFps6n5mXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **SPECIFICS:**\n",
        "Some lines of code are noted with SPECIFICS, relating to the nature of Q-learning rather than RL in general.\n",
        "If interested further, read [this online article](https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/)!"
      ],
      "metadata": {
        "id": "j8XaIxQz84zb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCukEJbk1XNU",
        "outputId": "5fdcc276-87cd-4ee9-f36c-a63d92bc403c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/925.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/925.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.22.4)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium)\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.7.1)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, jax-jumpy, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0\n"
          ]
        }
      ],
      "source": [
        "# First install dependencies as required:\n",
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium\n",
        "import numpy as np\n",
        "import random, time, math, tqdm"
      ],
      "metadata": {
        "id": "AM6lrE3d58zp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a folder which will store the qtables for us:\n",
        "!mkdir qtable"
      ],
      "metadata": {
        "id": "AILq7avl6BdK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setting the learning algorithm (Q-table) up**"
      ],
      "metadata": {
        "id": "k5iXqEue9YqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SPECIFICS: Can be skipped if not curious about Q-learning!\n",
        "# Q-table setup and roughly how it works.\n",
        "\n",
        "QTABLE_FOLDER_PATH = \"./qtable/\"\n",
        "DEFAULT_QTABLE_PATH = \"./qtable/default.npy/\"\n",
        "\n",
        "class Qtable:\n",
        "    NP_ARRAY_WIN_SIZE = np.array([0.25, 0.25, 0.01, 0.1])\n",
        "\n",
        "    # Creating a new Q table\n",
        "    def __init__(self):\n",
        "        # determines how small we chop the observation space\n",
        "        discretized_observation = [30, 30, 50, 60]\n",
        "        env = gymnasium.make(\"CartPole-v1\")\n",
        "        # setting up initial values for the table\n",
        "        self.Q = np.random.uniform(\n",
        "            low=0, high=1,\n",
        "            size=(discretized_observation + [env.action_space.n])\n",
        "            )\n",
        "        # print(Q.shape)\n",
        "\n",
        "    # \"state\" is discrete\n",
        "    def get_optimal_action(self, state):\n",
        "        return np.argmax(self.Q[state])\n",
        "\n",
        "    # \"state\" is discrete\n",
        "    def update_state_action_reward(self, state, action, reward):\n",
        "        self.Q[state + (action, )] = reward\n",
        "\n",
        "    # \"state\" is discrete\n",
        "    def get_reward(self, state, action):\n",
        "        return self.Q[state + (action, )]\n",
        "\n",
        "    # \"state\" is discrete\n",
        "    def get_best_reward(self, state):\n",
        "        return np.max(self.Q[state])\n",
        "\n",
        "    # \"state\" is discrete\n",
        "    def update_given_state_action_next_state_pairs_in_time_order(\n",
        "        self, s_a_n_s, rEpisode\n",
        "        ):\n",
        "        s_a_n_s.reverse()\n",
        "        for pairs in s_a_n_s:\n",
        "            s, a, n_s = pairs[0], pairs[1], pairs[2]\n",
        "            r = self.get_reward(s, a) + ALPHA * (rEpisode + GAMMA * (self.get_best_reward(n_s) - self.get_reward(s, a)))\n",
        "            self.update_state_action_reward(s, a, r)\n",
        "\n",
        "    # saves the table into a .npy file\n",
        "    def save_table(self, name=None):\n",
        "        time_of_creation = time.localtime(time.time())[0:5]\n",
        "        acc = str(time_of_creation[0])\n",
        "        [acc := acc + \"_\" + str(x) for x in time_of_creation[1:5]]\n",
        "        name_original = \"Cartpole_Q_table_\" + acc\n",
        "\n",
        "        if name is None:\n",
        "            name = name_original\n",
        "\n",
        "        name = QTABLE_FOLDER_PATH + \"/\" + name\n",
        "        np.save(name, self.Q)\n",
        "\n",
        "    # loads the .npy file table into a numpy array\n",
        "    def load_table(self, path=None):\n",
        "        if path is None:\n",
        "            path = DEFAULT_QTABLE_PATH\n",
        "\n",
        "        self.Q = np.load(path)\n",
        "\n",
        "    # Maps each continuous state vector obtained from env to a discrete state\n",
        "    # stored in Q table\n",
        "    # Divides the state array by the window sizes specified by ARRAY_WIN_SIZE,\n",
        "    # then adds the np.array to turn them to values above 0 (and hopefully\n",
        "    # within range of the Q table)\n",
        "    @staticmethod\n",
        "    def get_discrete_state(state):\n",
        "        discrete_state = state/Qtable.NP_ARRAY_WIN_SIZE + np.array([15,10,1,10])\n",
        "        return tuple(discrete_state.astype(int))"
      ],
      "metadata": {
        "id": "1Ol2mJpB7esn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparing the training code**"
      ],
      "metadata": {
        "id": "BpxCBtyP9Yfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPISODES = 60000\n",
        "ALPHA = 0.1 # learning rate\n",
        "GAMMA = 0.95 # discount rate\n",
        "INITIAL_EPSILON = 1 # probability for exploration\n",
        "EPSILON_DECAY_VALUE = 0.99995\n",
        "\n",
        "RENDER_TRAINING = False\n",
        "SAVE_TRAINING_RESULT = True\n",
        "TRAIN_AGENT = True\n",
        "\n",
        "#+++++++++++++++++++++++++++++++++++++++\n",
        "#Training\n",
        "\n",
        "def train():\n",
        "    # setup environment\n",
        "    r_m = \"human\" if RENDER_TRAINING else None\n",
        "    env = gymnasium.make(\"CartPole-v1\", render_mode=r_m) # setting environment up\n",
        "\n",
        "    # setup algorithm that learns what action to take.\n",
        "    # In our case, we use specifically the qtable for qlearning.\n",
        "    Q = Qtable()\n",
        "\n",
        "    prior_reward = 0\n",
        "    epsilon = INITIAL_EPSILON\n",
        "\n",
        "    # Single episode loop:\n",
        "    for episode in tqdm.tqdm(range(EPISODES)):#tqdm makes training progress bar\n",
        "        # training:\n",
        "        # 0) reset the environment (env), and setup appropriate values:\n",
        "        # - state of env\n",
        "        # - rewards over episode (0)\n",
        "        # - done or not? which is False\n",
        "        # - array storing pairs of state, action and next state required to\n",
        "        #   update the q-table\n",
        "        # - epsilon for epsilon-greedy, which decreases by a complex value each\n",
        "        #   episode (check detail in code)\n",
        "\n",
        "        s, _ = env.reset()\n",
        "        s_disc = Qtable.get_discrete_state(s)\n",
        "        rEpisode = 0\n",
        "        d = False\n",
        "\n",
        "        # SPECIFICS: Q-TABLE AND STORING STATE ACTION NEXT STATE PAIRS\n",
        "        # Pairs of state, action and next state in episode used at the episode's\n",
        "        # end to update values of each actions given each state (as related to\n",
        "        # entire reward in episode)\n",
        "        state_action_nextstate_pairs = []\n",
        "\n",
        "        # Training loop:\n",
        "        while not d:\n",
        "\n",
        "            # SPECIFICS: WHAT IS EPSILON-GREEDY?\n",
        "            # SPECIFICS: HOW IS THE OPTIMAL SOLUTION DETERMINED IN Q LEARNING?\n",
        "            # -) choose an action: if epsilon is greater than random value,\n",
        "            #    choose optimal solution so far\n",
        "            #    otherwise choose an action at random in the action space\n",
        "\n",
        "            optimal_action = Q.get_optimal_action(s_disc)\n",
        "            random_action = random.sample([0, 1], 1)[0]\n",
        "\n",
        "            threshold = random.random()\n",
        "            a = optimal_action if (threshold > epsilon) else random_action\n",
        "\n",
        "            # -) update the environment accordingly given the action, taking:\n",
        "            # new state, new reward, done?, info\n",
        "            n_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "            if terminated or truncated:\n",
        "                d = True\n",
        "\n",
        "            n_s_disc = Qtable.get_discrete_state(n_s)\n",
        "            state_action_nextstate_pairs.append([s_disc, a, n_s_disc])\n",
        "\n",
        "            # -) update info to adjust at the end of the step\n",
        "            s_disc = n_s_disc\n",
        "            rEpisode += r\n",
        "\n",
        "        # Then adjust epsilon accordingly once a single episode loop is over.\n",
        "        if epsilon > 0.05: #epsilon modification\n",
        "            if rEpisode > prior_reward and episode > 10000:\n",
        "                epsilon = math.pow(EPSILON_DECAY_VALUE, episode - 10000)\n",
        "\n",
        "        # Update qtable appropriately\n",
        "        # SPECIFICS: HOW IS THE QTABLE UPDATED?\n",
        "        Q.update_given_state_action_next_state_pairs_in_time_order(\n",
        "            state_action_nextstate_pairs, rEpisode\n",
        "            )\n",
        "\n",
        "    # End of training\n",
        "    env.close() # close the training env\n",
        "    if SAVE_TRAINING_RESULT:\n",
        "        Q.save_table()\n",
        "\n",
        "    return Q"
      ],
      "metadata": {
        "id": "Mq-mZk1H1kk7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparing the evaluation code**"
      ],
      "metadata": {
        "id": "M1czZyhv9qdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#+++++++++++++++++++++++++++++++++++++++\n",
        "#Evaluating\n",
        "\n",
        "def evaluate(qtable=None, path=None):\n",
        "    if qtable is None:\n",
        "        Q = Qtable()\n",
        "        Q.load_table(path)\n",
        "    else:\n",
        "        Q = qtable\n",
        "\n",
        "    # Evaluation loop:\n",
        "    env_eval = gymnasium.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "    s, _ = env_eval.reset() #reset the environment\n",
        "\n",
        "    s_disc = Qtable.get_discrete_state(s)\n",
        "    terminated = truncated = False\n",
        "\n",
        "    while not (terminated or truncated):\n",
        "\n",
        "        # get optimal action by agent\n",
        "        a = Q.get_optimal_action(s_disc)\n",
        "\n",
        "        # update env accordingly\n",
        "        s, _, terminated, truncated, _ = env_eval.step(a)\n",
        "        s_disc = Qtable.get_discrete_state(s)\n",
        "\n",
        "    env_eval.close()"
      ],
      "metadata": {
        "id": "g_1-KnMZ9rTw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Great code from here[https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553]!\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"\n",
        "Ensure you have imagemagick installed with\n",
        "sudo apt-get install imagemagick\n",
        "\n",
        "Open file in CLI with:\n",
        "xgd-open <filelname>\n",
        "\"\"\"\n",
        "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
        "\n",
        "    #Mess with this to change frame size\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
        "\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    anim.save(path + filename, writer='imagemagick', fps=60)\n",
        "\n",
        "def evaluate_while_returning_gif(qtable=None, path=None):\n",
        "    if qtable is None:\n",
        "        Q = Qtable()\n",
        "        Q.load_table(path)\n",
        "    else:\n",
        "        Q = qtable\n",
        "\n",
        "    # stores frames to be stored as gif\n",
        "    frames = []\n",
        "\n",
        "    # Evaluation loop:\n",
        "    env_eval = gymnasium.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "    s, _ = env_eval.reset() #reset the environment\n",
        "\n",
        "    s_disc = Qtable.get_discrete_state(s)\n",
        "    terminated = truncated = False\n",
        "\n",
        "    while not (terminated or truncated):\n",
        "\n",
        "        # get optimal action by agent\n",
        "        a = Q.get_optimal_action(s_disc)\n",
        "\n",
        "        # update env accordingly\n",
        "        s, _, terminated, truncated, _ = env_eval.step(a)\n",
        "\n",
        "        #Render to frames buffer\n",
        "        frames.append(env_eval.render())\n",
        "\n",
        "        s_disc = Qtable.get_discrete_state(s)\n",
        "\n",
        "    env_eval.close()\n",
        "    save_frames_as_gif(frames)"
      ],
      "metadata": {
        "id": "GqQJ9awXTHH9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Executing the codes**"
      ],
      "metadata": {
        "id": "sd8pQieR9zKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First train, details in train function.\n",
        "# +++++++++++++++++++++++++++++++++\n",
        "# + TAKES ABOUT 6 MIN TO TRAIN!!! +\n",
        "# +++++++++++++++++++++++++++++++++\n",
        "\n",
        "if TRAIN_AGENT:\n",
        "    qtable = train()\n",
        "\n",
        "# Evaluation. See it in action, and some metrics! (TODO ADD METRICS! TO COME).\n",
        "# DOES UNFORTUNATELY NOT WORK WITH GOOGLE COLAB; SAVE RESULTS AS GIF TO SEE!\n",
        "# evaluate(qtable=qtable)\n",
        "\n",
        "# If we want to evaluate a specific saved q-table, use the code below\n",
        "# DOES UNFORTUNATELY NOT WORK WITH GOOGLE COLAB; SAVE RESULTS AS GIF TO SEE!\n",
        "# evaluate(path=\"qtable\\Cartpole_best_performing.npy\")\n",
        "\n",
        "# If we want to save the result as gif, run below\n",
        "# evaluate_while_returning_gif(qtable=qtable)\n",
        "# evaluate_while_returning_gif(path=\"/content/qtable/FILL_WITH_ACTUAL_QTABLE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "XHT900cc93J-",
        "outputId": "91e8b882-c33a-4355-854e-0decbbc1ebcf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.animation:MovieWriter imagemagick unavailable; using Pillow instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}