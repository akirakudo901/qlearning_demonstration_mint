{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfvmcCR02QER"
      },
      "source": [
        "# **Reinforcement learning demonstration**\n",
        "This notebook demonstrates the **basic training loop** for **reinforcement learning** through **q-learning** and the **epsilon-greedy algorithm**.\n",
        "Tried to make it as brief as possible, so please ask anything you're unsure about! \n",
        "\n",
        "____________________________________________"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HFiFps6n5mXB"
      },
      "source": [
        "## **WHAT IS REINFORCEMENT LEARNING?**\n",
        "\n",
        "### The four features of reinforcement learning (RL)\n",
        "\n",
        "RL comes down to **four** features:\n",
        "* **Environment** - the world that we want to learn from. A single situation in an environment is a **\"state\"**.\n",
        "* **Agent** - the entity that learns through observing the environment and taking actions.\n",
        "* **Observation** - what the agent perceives about the world.\n",
        "* **Action** - what the agent does.\n",
        "______________________________________________\n",
        "\n",
        "### Agent-environment interaction (step)\n",
        "\n",
        "*A nice [16 second illustration](https://www.youtube.com/watch?v=-uXVu0l8guo&t=12s) of the flow on YouTube!*\n",
        "\n",
        "We represent a single **\"step\"** of agent-environment interaction as follows:\n",
        "1. the agent **observes** the environment\n",
        "2. it **chooses action** based on observation \n",
        "3. the action **modifies** the environment (& environment might also change naturally)\n",
        "\n",
        "In the next step, the agent observes the new environment state, takes action, \n",
        "which modifies the environment, and so on.\n",
        "A step is a single time step in the agent's view.\n",
        "\n",
        "______________________________________________\n",
        "\n",
        "### Reward function and policy learning\n",
        "\n",
        "The agent **learns from interacting** with the world:\n",
        "* a **\"reward function\"** inherent in the environment and hidden from the agent, evaluates **each state & action's rewarding-ness** to the agent in **numerical values**\n",
        "* the agent learns through **associating actions** taken in certain states with **resultant rewards**. \n",
        "Different algorithms (Q-learning, deep neural networks, etc.) are used for the agent's learning\n",
        "* the agent **starts off knowing nothing about the world**, and explores the world **taking random actions**\n",
        "* its goal is to **learn a guiding rule** for **action maximizing reward** at each state (called **\"policy\"**)\n",
        "\n",
        "Our goal is to let the agent **learn a highly performant policy**.\n",
        "Usual RL training involves the agent interacting a lot with the environment.\n",
        "______________________________________________\n",
        "\n",
        "### What changes from problem to problem for RL\n",
        "\n",
        "We change:\n",
        "* **ENVIRONMENT** : the world we learn from depends on the problem + (also includes observations and actions)\n",
        "  \n",
        "  <- probably the **software & signal processing team**'s field! \n",
        "\n",
        "* **REWARD FUNCTION** : what quantifies the goal of the agent\n",
        "  \n",
        "  <- probably the **whole team**'s field! (maybe more on the deep learning team?)\n",
        "\n",
        "* **AGENT'S LEARNING ALGORITHM** : the algorithm by which the agent learns\n",
        "  \n",
        "  <- probably the **deep learning team**'s field!\n",
        "\n",
        "The general flow of problem solving stays the same for most RL problems!\n",
        "\n",
        "_____________________________________"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j8XaIxQz84zb"
      },
      "source": [
        "#### **SPECIFICS:**\n",
        "Some lines of code are noted with SPECIFICS, relating to the nature of Q-learning rather than RL in general.\n",
        "If interested further, read [this online article](https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCukEJbk1XNU",
        "outputId": "446aa376-f25f-4cf7-b231-88dc9b4b9a07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in c:\\users\\mashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (1.24.2)\n",
            "Requirement already satisfied: gymnasium-notices>=0.0.1 in c:\\users\\mashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (0.0.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: jax-jumpy>=0.2.0 in c:\\users\\mashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mashi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (4.4.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# First install dependencies as required:\n",
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AM6lrE3d58zp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mashi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import gymnasium\n",
        "import numpy as np\n",
        "import random, time, math, tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AILq7avl6BdK",
        "outputId": "186e7faf-25a8-4a21-c54b-90ee8516e16c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "�T�u�f�B���N�g���܂��̓t�@�C�� qtable �͊��ɑ��݂��܂��B\n"
          ]
        }
      ],
      "source": [
        "# Make a folder which will store the qtables for us:\n",
        "!mkdir qtable"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k5iXqEue9YqP"
      },
      "source": [
        "## **Setting the learning algorithm (Q-table) up**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1Ol2mJpB7esn"
      },
      "outputs": [],
      "source": [
        "# SPECIFICS: Can be skipped if not curious about Q-learning!\n",
        "# Q-table setup and roughly how it works.\n",
        "\n",
        "QTABLE_FOLDER_PATH = \"./qtable/\"\n",
        "DEFAULT_QTABLE_PATH = \"./qtable/default.npy/\"\n",
        "\n",
        "class Qtable:\n",
        "    NP_ARRAY_WIN_SIZE = np.array([0.25, 0.25, 0.01, 0.1])\n",
        "\n",
        "    # Creating a new Q table\n",
        "    def __init__(self):\n",
        "        # determines how small we chop the observation space\n",
        "        discretized_observation = [30, 30, 50, 60] \n",
        "        env = gymnasium.make(\"CartPole-v1\")\n",
        "        # setting up initial values for the table\n",
        "        self.Q = np.random.uniform(low=0, high=1, size=(discretized_observation + [env.action_space.n]))\n",
        "        # print(Q.shape)\n",
        "    \n",
        "    # \"state\" is discrete\n",
        "    def get_optimal_action(self, state):\n",
        "        return np.argmax(self.Q[state])\n",
        "    \n",
        "    # \"state\" is discrete\n",
        "    def update_state_action_reward(self, state, action, reward):\n",
        "        self.Q[state + (action, )] = reward\n",
        "\n",
        "    # \"state\" is discrete\n",
        "    def get_reward(self, state, action):\n",
        "        return self.Q[state + (action, )]\n",
        "    \n",
        "    # \"state\" is discrete\n",
        "    def get_best_reward(self, state):\n",
        "        return np.max(self.Q[state])\n",
        "    \n",
        "    # \"state\" is discrete\n",
        "    def update_given_state_action_next_state_pairs_in_time_order(self, s_a_n_s, rEpisode):\n",
        "        s_a_n_s.reverse()\n",
        "        for pairs in s_a_n_s:\n",
        "            s, a, n_s = pairs[0], pairs[1], pairs[2]\n",
        "            r = self.get_reward(s, a) + ALPHA * (rEpisode + GAMMA * (self.get_best_reward(n_s) - self.get_reward(s, a)))\n",
        "            self.update_state_action_reward(s, a, r)\n",
        "\n",
        "    # saves the table into a .npy file\n",
        "    def save_table(self, name=None):\n",
        "        time_of_creation = time.localtime(time.time())[0:5]\n",
        "        acc = str(time_of_creation[0])\n",
        "        [acc := acc + \"_\" + str(x) for x in time_of_creation[1:5]]\n",
        "        name_original = \"Cartpole_Q_table_\" + acc\n",
        "        \n",
        "        if name is None:\n",
        "            name = name_original\n",
        "\n",
        "        name = QTABLE_FOLDER_PATH + \"/\" + name\n",
        "        np.save(name, self.Q)\n",
        "    \n",
        "    # loads the .npy file table into a numpy array\n",
        "    def load_table(self, path=None):\n",
        "        if path is None:\n",
        "            path = DEFAULT_QTABLE_PATH\n",
        "\n",
        "        self.Q = np.load(path) \n",
        "    \n",
        "    # Maps each continuous state vector obtained from env to a discrete state stored in Q table\n",
        "    # Divides the state array by the window sizes specified by ARRAY_WIN_SIZE, then adds the np.array\n",
        "    # to turn them to values above 0 (and hopefully within range of the Q table)\n",
        "    @staticmethod\n",
        "    def get_discrete_state(state):\n",
        "        discrete_state = state/Qtable.NP_ARRAY_WIN_SIZE + np.array([15,10,1,10])\n",
        "        return tuple(discrete_state.astype(int))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BpxCBtyP9Yfr"
      },
      "source": [
        "## **Preparing the training code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Mq-mZk1H1kk7"
      },
      "outputs": [],
      "source": [
        "EPISODES = 60000\n",
        "ALPHA = 0.1 # learning rate\n",
        "GAMMA = 0.95 # discount rate\n",
        "INITIAL_EPSILON = 1 # probability for exploration\n",
        "EPSILON_DECAY_VALUE = 0.99995\n",
        "\n",
        "RENDER_TRAINING = False\n",
        "SAVE_TRAINING_RESULT = True\n",
        "TRAIN_AGENT = True\n",
        "\n",
        "#+++++++++++++++++++++++++++++++++++++++\n",
        "#Training\n",
        "\n",
        "def train():\n",
        "    # setup environment\n",
        "    r_m = \"human\" if RENDER_TRAINING else None \n",
        "    env = gymnasium.make(\"CartPole-v1\", render_mode=r_m) # setting the environment up\n",
        "\n",
        "    # setup algorithm that learns what action to take.\n",
        "    # In our case, we use specifically the qtable for qlearning.\n",
        "    Q = Qtable()\n",
        "\n",
        "    prior_reward = 0\n",
        "    epsilon = INITIAL_EPSILON\n",
        "\n",
        "    # Single episode loop:\n",
        "    for episode in tqdm.tqdm(range(EPISODES)): # tqdm allows us to make the progress bar during training\n",
        "        # training:\n",
        "        # 0) reset the environment (env), and setup appropriate values: \n",
        "        # - state of env\n",
        "        # - rewards over episode (0)\n",
        "        # - done or not? which is False\n",
        "        # - array storing pairs of state, action and next state required to update the q-table\n",
        "        # - epsilon for epsilon-greedy, which decreases by a complex value each episode (check detail in code)\n",
        "\n",
        "        s, _ = env.reset()\n",
        "        s_disc = Qtable.get_discrete_state(s)\n",
        "        rEpisode = 0\n",
        "        d = False\n",
        "\n",
        "        # SPECIFICS: Q-TABLE AND STORING STATE ACTION NEXT STATE PAIRS\n",
        "        # Pairs of state, action and next state in episode used at the episode's end to \n",
        "        # update values of each actions given each state (as related to entire reward in episode)\n",
        "        state_action_nextstate_pairs = [] \n",
        "\n",
        "        # Training loop:\n",
        "        while not d:\n",
        "\n",
        "            # SPECIFICS: WHAT IS EPSILON-GREEDY?\n",
        "            # SPECIFICS: HOW IS THE OPTIMAL SOLUTION DETERMINED IN Q LEARNING? \n",
        "            # -) choose an action: if epsilon is greater than random value, choose optimal solution so far\n",
        "            #    otherwise choose an action at random in the action space\n",
        "            \n",
        "            optimal_action = Q.get_optimal_action(s_disc) \n",
        "            random_action = random.sample([0, 1], 1)[0]\n",
        "\n",
        "            threshold = random.random()\n",
        "            a = optimal_action if (threshold > epsilon) else random_action\n",
        "\n",
        "            # -) update the environment accordingly given the action, taking: \n",
        "            # new state, new reward, done?, info\n",
        "            n_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "            if terminated or truncated:\n",
        "                d = True\n",
        "        \n",
        "            n_s_disc = Qtable.get_discrete_state(n_s)\n",
        "            state_action_nextstate_pairs.append([s_disc, a, n_s_disc])\n",
        "\n",
        "            # -) update info to adjust at the end of the step\n",
        "            s_disc = n_s_disc\n",
        "            rEpisode += r\n",
        "        \n",
        "        # Then adjust epsilon accordingly once a single episode loop is over.\n",
        "        if epsilon > 0.05: #epsilon modification\n",
        "            if rEpisode > prior_reward and episode > 10000:\n",
        "                epsilon = math.pow(EPSILON_DECAY_VALUE, episode - 10000)\n",
        "        \n",
        "        # Update qtable appropriately\n",
        "        # SPECIFICS: HOW IS THE QTABLE UPDATED?\n",
        "        Q.update_given_state_action_next_state_pairs_in_time_order(state_action_nextstate_pairs, rEpisode)\n",
        "\n",
        "    # End of training\n",
        "    env.close() # close the training env\n",
        "    if SAVE_TRAINING_RESULT:\n",
        "        Q.save_table()\n",
        "\n",
        "    return Q"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M1czZyhv9qdM"
      },
      "source": [
        "## **Preparing the evaluation code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g_1-KnMZ9rTw"
      },
      "outputs": [],
      "source": [
        "#+++++++++++++++++++++++++++++++++++++++\n",
        "#Evaluating\n",
        "\n",
        "def evaluate(qtable=None, path=None):\n",
        "    if qtable is None:\n",
        "        Q = Qtable()\n",
        "        Q.load_table(path)\n",
        "    else:\n",
        "        Q = qtable\n",
        "\n",
        "    # Evaluation loop:\n",
        "    env_eval = gymnasium.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "    s, _ = env_eval.reset() #reset the environment\n",
        "\n",
        "    s_disc = Qtable.get_discrete_state(s)\n",
        "    terminated = truncated = False\n",
        "\n",
        "    while not (terminated or truncated):\n",
        "\n",
        "        # get optimal action by agent\n",
        "        a = Q.get_optimal_action(s_disc)\n",
        "\n",
        "        # update env accordingly\n",
        "        s, _, terminated, truncated, _ = env_eval.step(a)\n",
        "        s_disc = Qtable.get_discrete_state(s)\n",
        "\n",
        "    env_eval.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Great code from here[https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553]!\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"\n",
        "Ensure you have imagemagick installed with \n",
        "sudo apt-get install imagemagick\n",
        "\n",
        "Open file in CLI with:\n",
        "xgd-open <filelname>\n",
        "\"\"\"\n",
        "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
        "\n",
        "    #Mess with this to change frame size\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
        "\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    anim.save(path + filename, writer='imagemagick', fps=60)\n",
        "\n",
        "def evaluate_while_returning_gif(qtable=None, path=None):\n",
        "    if qtable is None:\n",
        "        Q = Qtable()\n",
        "        Q.load_table(path)\n",
        "    else:\n",
        "        Q = qtable\n",
        "\n",
        "    # stores frames to be stored as gif\n",
        "    frames = []\n",
        "\n",
        "    # Evaluation loop:\n",
        "    env_eval = gymnasium.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "    s, _ = env_eval.reset() #reset the environment\n",
        "\n",
        "    s_disc = Qtable.get_discrete_state(s)\n",
        "    terminated = truncated = False\n",
        "\n",
        "    while not (terminated or truncated):\n",
        "\n",
        "        # get optimal action by agent\n",
        "        a = Q.get_optimal_action(s_disc)\n",
        "\n",
        "        # update env accordingly\n",
        "        s, _, terminated, truncated, _ = env_eval.step(a)\n",
        "\n",
        "        #Render to frames buffer\n",
        "        frames.append(env_eval.render())\n",
        "\n",
        "        s_disc = Qtable.get_discrete_state(s)\n",
        "\n",
        "    env_eval.close()\n",
        "    save_frames_as_gif(frames)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sd8pQieR9zKj"
      },
      "source": [
        "## **Executing the codes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "XHT900cc93J-",
        "outputId": "c8192086-cbdd-4cbf-c4bb-3953b9374fd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MovieWriter imagemagick unavailable; using Pillow instead.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#First train, details in train function.\n",
        "if TRAIN_AGENT:\n",
        "    qtable = train()\n",
        "\n",
        "# Evaluation. See it in action, and some metrics! (TODO ADD METRICS! TO COME).\n",
        "# evaluate(qtable=qtable)\n",
        "\n",
        "# If we want to evaluate a specific saved q-table, use the code below\n",
        "# evaluate(path=\"qtable\\Cartpole_best_performing.npy\")\n",
        "\n",
        "# If we want to save the result as gif, run below\n",
        "evaluate_while_returning_gif(path=\"qtable\\Cartpole_best_performing.npy\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
